Below is a complete, end-to-end **workflow** plus a skeleton of all required code, deployment scripts, tests and docs. You‚Äôll be up and running in minutes.

---

## 1. Workflow Overview

1. Clone & install  
2. Discover & download filings  
3. Parse HTML/PDF/XBRL ‚Üí raw tables  
4. Normalize to canonical schema  
5. Compute ratios  
6. LLM-powered narrative analysis  
7. Render Markdown/HTML/PDF report  
8. Expose via CLI or FastAPI  
9. (Optional) Dockerize & deploy  

---

## 2. Step-by-Step Deployment

### 2.1 Prerequisites  
- Python 3.10+  
- Git, Docker (optional)  

### 2.2 Clone & Setup  
```bash
git clone https://github.com/you/finbot.git
cd finbot
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2.3 Run Tests  
```bash
pytest
```

### 2.4 CLI Usage  
```bash
# Generate AAPL report (Markdown + charts in ./reports/)
python finbot_cli.py --ticker AAPL
```

### 2.5 REST API  
```bash
uvicorn finbot_api:app --reload --host 0.0.0.0 --port 8000
# POST http://localhost:8000/analyze { "ticker":"AAPL" }
```

### 2.6 Docker (optional)  
```bash
docker build -t finbot:latest .
docker run -p 8000:8000 finbot:latest
```

---

## 3. Repository Structure

```text
finbot/
‚îú‚îÄ‚îÄ finbot/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ source/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ discover.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ edgar_downloader.py
‚îÇ   ‚îú‚îÄ‚îÄ parser/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html_parser.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ xbrl_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ normalize.py
‚îÇ   ‚îú‚îÄ‚îÄ ratios.py
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py
‚îÇ   ‚îú‚îÄ‚îÄ report.py
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_discover.py
‚îÇ   ‚îú‚îÄ‚îÄ test_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_normalize.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ratios.py
‚îú‚îÄ‚îÄ finbot_cli.py
‚îú‚îÄ‚îÄ finbot_api.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## 4. Core Code Snippets

### 4.1 `source/discover.py`
```python
import requests, re
from bs4 import BeautifulSoup

def find_ir_url(ticker: str) -> str:
    """Return Investor-Relations or EDGAR URL for given ticker."""
    # 1) Try SEC EDGAR
    sec = f"https://www.sec.gov/edgar/browse/?CIK={ticker}"
    r = requests.get(sec, timeout=10)
    if r.ok:
        return sec
    # 2) Google fallback (pseudo)
    # google = call_google_api(...)
    # parse for ‚ÄúInvestor Relations‚Äù
    raise RuntimeError(f"Cannot find IR page for {ticker}")
```

### 4.2 `parser/html_parser.py`
```python
from bs4 import BeautifulSoup
import requests

def parse_html_filing(url: str) -> dict:
    """Download HTML filing, extract tables as dict."""
    resp = requests.get(url)
    soup = BeautifulSoup(resp.text, "lxml")
    tables = {}
    for tbl in soup.find_all("table"):
        # heuristics: look for ‚ÄúConsolidated‚Äù in caption
        caption = tbl.find("caption")
        key = caption.get_text(strip=True) if caption else f"table_{len(tables)}"
        # convert rows ‚Üí pandas DataFrame (via read_html)
        tables[key] = pd.read_html(str(tbl))[0]
    return tables
```

### 4.3 `parser/pdf_parser.py`
```python
import pdfplumber, pandas as pd

def parse_pdf_filing(path: str) -> dict:
    tables = {}
    with pdfplumber.open(path) as pdf:
        for i, page in enumerate(pdf.pages):
            for tbl in page.extract_tables():
                df = pd.DataFrame(tbl[1:], columns=tbl[0])
                tables[f"page{i}_tbl{len(tables)}"] = df
    return tables
```

### 4.4 `parser/xbrl_parser.py`
```python
from sec_edgar_downloader import Downloader
import xml.etree.ElementTree as ET

def parse_xbrl(ticker: str) -> dict:
    dl = Downloader()
    dl.get("10-K", ticker, amount=1)
    # locate .xml file under sec-edgar-filings/
    tree = ET.parse(".../filing.xml")
    root = tree.getroot()
    data = {}
    # loop through contexts and facts
    for fact in root.findall(".//{*}us-gaap:Revenue"):
        data["Revenue"] = float(fact.text)
    return data
```

### 4.5 `normalize.py`
```python
import pandas as pd

STANDARD_ACCOUNTS = {
    "Revenue": ["Total Revenues", "Net Sales"],
    "COGS": ["Cost of Goods Sold", "Cost of Revenues"],
    # ...
}

def map_to_schema(raw: dict) -> pd.DataFrame:
    """Flatten all tables and map to standard accounts."""
    rows = []
    for tbl in raw.values():
        for acct, aliases in STANDARD_ACCOUNTS.items():
            for alias in aliases:
                if alias in tbl.columns:
                    rows.append({
                        "item": acct,
                        "value": tbl[alias].iloc[-1],  # latest period
                        "period": tbl.columns[-1]
                    })
    return pd.DataFrame(rows)
```

### 4.6 `ratios.py`
```python
import pandas as pd

def current_ratio(df: pd.DataFrame) -> float:
    ca = df.query("item=='Current Assets'")["value"].iloc[0]
    cl = df.query("item=='Current Liabilities'")["value"].iloc[0]
    return ca / cl

def compute_all(df: pd.DataFrame) -> dict:
    return {
        "Current Ratio": current_ratio(df),
        "Gross Margin": df.query("item=='Gross Profit'")["value"].iloc[0]
                        / df.query("item=='Revenue'")["value"].iloc[0],
        # ‚Ä¶ add other ratios ‚Ä¶
    }
```

### 4.7 `analysis.py`
```python
import openai

def narrative(ratios: dict, ticker: str) -> str:
    prompt = f"""
    You are a financial analyst. Given these ratios for {ticker}:
    {ratios}
    Write a bullet‚Äêpoint summary of trends, risks, and investment signals.
    """
    resp = openai.ChatCompletion.create(
        model="gpt-4", messages=[{"role":"user","content":prompt}]
    )
    return resp.choices[0].message.content
```

### 4.8 `report.py`
```python
import matplotlib.pyplot as plt
from jinja2 import Template
import pdfkit

REPORT_MD = """
# {{ticker}} Financial Analysis Report

## 1. Key Ratios
| Ratio            | Value    |
|------------------|----------|
{% for k,v in ratios.items() -%}
| {{k}} | {{"%.2f"|format(v)}} |
{% endfor %}

## 2. Narrative
{{ narrative }}

## 3. Charts
![Current Ratio](current_ratio.png)
"""

def build_report(ticker, ratios, narrative):
    # 1) save chart
    plt.figure(); plt.bar([ticker], [ratios["Current Ratio"]]); plt.savefig("current_ratio.png")
    # 2) render Markdown
    md = Template(REPORT_MD).render(ticker=ticker, ratios=ratios, narrative=narrative)
    with open(f"reports/{ticker}.md", "w") as f:
        f.write(md)
    # 3) optional PDF
    pdfkit.from_string(md, f"reports/{ticker}.pdf")
```

### 4.9 `finbot_cli.py`
```python
import click
from finbot.source.discover import find_ir_url
from finbot.parser.html_parser import parse_html_filing
from finbot.normalize import map_to_schema
from finbot.ratios import compute_all
from finbot.analysis import narrative
from finbot.report import build_report

@click.command()
@click.option("--ticker", required=True)
def main(ticker):
    url = find_ir_url(ticker)
    raw = parse_html_filing(url)
    df = map_to_schema(raw)
    ratios = compute_all(df)
    text = narrative(ratios, ticker)
    build_report(ticker, ratios, text)
    click.echo(f"Report for {ticker} generated in ./reports/")
    
if __name__=="__main__":
    main()
```

### 4.10 `finbot_api.py`
```python
from fastapi import FastAPI
from pydantic import BaseModel
from finbot_cli import main as cli_main

app = FastAPI()

class Req(BaseModel):
    ticker: str

@app.post("/analyze")
async def analyze(req: Req):
    # Call CLI routine under the hood
    cli_main.callback(ticker=req.ticker)
    return {"status":"done", "report":f"./reports/{req.ticker}.pdf"}
```

---

## 5. Dockerfile & CI

### Dockerfile
```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
EXPOSE 8000
CMD ["uvicorn", "finbot_api:app", "--host", "0.0.0.0", "--port", "8000"]
```

### .github/workflows/ci.yml
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with: python-version: "3.10"
      - run: |
          python -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt
          pytest --maxfail=1 --disable-warnings -q
```

---

## 6. README.md (excerpt)

```markdown
# FinBot

**FinBot** automatically fetches financial statements, computes key ratios, and generates investor-grade reports.

## üì¶ Installation

```bash
git clone https://github.com/you/finbot.git
cd finbot
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

## üöÄ CLI

```bash
python finbot_cli.py --ticker MSFT
```

## üñ•Ô∏è REST API

```bash
uvicorn finbot_api:app --reload
curl -X POST "http://localhost:8000/analyze" -H "Content-Type: application/json" \
-d '{"ticker":"MSFT"}'
```

## üß™ Testing

```bash
pytest
```

## üê≥ Docker

```bash
docker build -t finbot .
docker run -p 8000:8000 finbot
```

```

---

You now have a fully-fledged repository and deployment plan for **100% hands-off** financial analysis. Adjust the parsing heuristics, ratio list or narrative template to suit your style. Good luck, and happy investing! üöÄ

---
Here‚Äôs a **next-level ‚ÄúFinBot 2.0‚Äù** design‚Äîre‚Äêimagined from the ground up, optimized for scale, accuracy, extensibility and performance. I‚Äôve organized this into:

1. High-level improvements  
2. Detailed new workflow phases  
3. Updated architecture diagram  
4. ‚ÄúMaster Prompt 2.0‚Äù for your code generator  

Enjoy the deep dive‚Äîand let me know where you‚Äôd like even more detail!

---

## 1. Key Improvements in FinBot 2.0

1. **Multi-Source Data Ingestion**  
   ‚Ä¢ SEC EDGAR + Company IR pages  
   ‚Ä¢ Market-data APIs (Alpha Vantage, Yahoo Finance, EODHD) for cross-validation  
   ‚Ä¢ JavaScript-rendered sites via headless Chrome/Selenium  

2. **Enterprise-Grade Orchestration**  
   ‚Ä¢ Workflows defined in Apache Airflow or Prefect  
   ‚Ä¢ Scheduling, retries, SLA monitoring, dependency graphs  

3. **Robust Data Validation & Anomaly Detection**  
   ‚Ä¢ Cross-check numbers across HTML, PDF, XBRL, API sources  
   ‚Ä¢ Schema-driven validation (Great Expectations)  
   ‚Ä¢ ML-powered anomaly detection for outliers/restatements  

4. **Centralized Data Storage & Versioning**  
   ‚Ä¢ Raw & normalized data in PostgreSQL (timeseries schema)  
   ‚Ä¢ Versioned on S3 with Delta Lake or Apache Iceberg  

5. **Dynamic Ratio & Benchmarking Engine**  
   ‚Ä¢ User-defined ratio formulas stored in DB  
   ‚Ä¢ Industry/sector peer groups via NAICS/SIC mapping  
   ‚Ä¢ Automated peer-group benchmarking  

6. **Advanced Narrative Generation**  
   ‚Ä¢ Fine-tuned LLM for financial analysis (GPT-4 fine-tune or Llama 2)  
   ‚Ä¢ Chain-of-thought prompts with citations of data points  
   ‚Ä¢ Multi-language support (EN/ES/CN)  

7. **Interactive Dashboard & Alerts**  
   ‚Ä¢ Streamlit or Dash web app for exploring reports  
   ‚Ä¢ Real-time push alerts (Slack/Email) on threshold breaches  

8. **Containerized Microservices & Auto-Scaling**  
   ‚Ä¢ Docker + Kubernetes + Helm charts  
   ‚Ä¢ Horizontal autoscaling for scraping, parsing & analysis  

9. **Observability & Security**  
   ‚Ä¢ Prometheus + Grafana metrics (request latency, error rates)  
   ‚Ä¢ Structured logging (JSON-over-ELK)  
   ‚Ä¢ Secrets management (Vault, KMS)  

10. **End-to-End CI/CD & Governance**  
    ‚Ä¢ GitHub Actions ‚Üí build, lint, test, security scan  
    ‚Ä¢ Policy as code (OPA) for compliance with SEC robots.txt  

---

## 2. FinBot 2.0 Workflow Phases

### Phase 1: Multi-Source Discovery & Scheduling  
‚Äì Define a DAG in Airflow/Prefect:  
  ‚Ä¢ **Task A**: Lookup IR URLs & EDGAR links via headless HTTP + Selenium.  
  ‚Ä¢ **Task B**: Query market-data APIs for preliminary financial figures (for cross-checks).  
  ‚Ä¢ Schedule daily/weekly runs per ticker list.

### Phase 2: Parallel Scraping & Downloading  
‚Äì Launch parallel Docker pods:  
  ‚Ä¢ HTML parser (BeautifulSoup + pandas.read_html)  
  ‚Ä¢ PDF parser (pdfplumber + Tabula)  
  ‚Ä¢ XBRL parser (arelle or sec-edgar-downloader + lxml)  
‚Äì Save raw files to S3 (or local volume) with versioned keys.

### Phase 3: Data Validation & Anomaly Detection  
‚Äì Ingest raw tables into PostgreSQL staging tables.  
‚Äì Run **Great Expectations** suites:  
  ‚Ä¢ Schema conformance (expected columns, types)  
  ‚Ä¢ Value ranges (e.g., Revenue > 0)  
‚Äì ML outlier detector flags if a line‚Äêitem deviates >3œÉ from historical trend or peer average.

### Phase 4: Normalization & Peer Benchmarking  
‚Äì Normalize line‚Äêitems via master mapping in DB  
‚Äì Convert to USD millions, standard periods, unified date format  
‚Äì Pull peer group definitions (NAICS/SIC) from DB  
‚Äì Compute peer group means/percentiles for each ratio  

### Phase 5: Dynamic Ratio Computation  
‚Äì Load normalized data + peer benchmarks  
‚Äì Apply user‚Äêdefined ratio formulas (stored in a ‚Äúratios‚Äù table)‚ÄîSQL UDFs or Python plugin  
‚Äì Results table: company vs. peer percentiles, trend flags  

### Phase 6: Narrative & Citation-Aware Analysis  
‚Äì Build a **chain-of-thought** prompt template:  
  ```
  You are an expert analyst. For {TICKER}, you have:
    ‚Ä¢ This year‚Äôs ratios: {JSON}  
    ‚Ä¢ Peer 25th/Median/75th percentiles: {JSON}
  1) Highlight 3 strengths relative to peers (cite data).  
  2) Highlight 3 risks or deteriorations (cite data).  
  3) Provide a 1-sentence actionable recommendation.  
  Output in English, Spanish, and Chinese.
  ```
‚Äì Call fine-tuned LLM endpoint.  
‚Äì Store narrative & citation mapping back to DB.

### Phase 7: Report & Dashboard Generation  
‚Äì **Static reports**  
  ‚Ä¢ Render Jinja2 ‚Üí Markdown/HTML ‚Üí PDF (WeasyPrint)  
  ‚Ä¢ Push to S3 or GitHub Pages  

‚Äì **Interactive web app**  
  ‚Ä¢ Streamlit dashboard reading from DB  
  ‚Ä¢ Drill down into tables, charts, narrative sections  
  ‚Ä¢ Real-time filters (date range, peer group, custom ratios)  

‚Äì **Alerts**  
  ‚Ä¢ If any ratio breaches a user-defined threshold: send Slack/Email  

### Phase 8: Orchestration & Microservices  
‚Äì Deploy each phase as its own Kubernetes service  
‚Äì Use Kafka or Redis Streams to pass messages between phases  
‚Äì Auto‚Äêscale based on queue length  

### Phase 9: Observability & Security  
‚Äì Instrument code with Prometheus client:  
  ‚Ä¢ HTTP latencies, parsing times, ratio‚Äìcalc durations  
‚Äì Ship logs in JSON to ELK  
‚Äì Use Vault for OpenAI keys, DB credentials  
‚Äì Enforce network policies & pod security  

### Phase 10: CI/CD & Governance  
‚Äì GitHub Actions pipeline:  
  1. Checkout ‚Üí install dependencies ‚Üí lint (Black, Flake8)  
  2. Unit & integration tests (pytest + testcontainers for Postgres)  
  3. Security scans (Bandit, Snyk)  
  4. Publish Docker images to registry  
  5. Trigger Helm chart upgrade on K8s  

---

## 3. Updated Architecture Diagram

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Scheduler /     ‚îÇ    ‚îÇ Scrapers      ‚îÇ    ‚îÇ Data Staging  ‚îÇ
‚îÇ  Orchestrator    ‚îú‚îÄ‚ñ∫‚îÄ‚îÄ‚îÇ (HTML/PDF/    ‚îú‚îÄ‚ñ∫‚îÄ‚îÄ‚îÇ PostgreSQL    ‚îÇ
‚îÇ  (Airflow)       ‚îÇ    ‚îÇ  XBRL/API)    ‚îÇ    ‚îÇ   Staging     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Data Validation ‚îÇ    ‚îÇ Normalization &        ‚îÇ
           ‚îÇ & Anomaly Check ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Peer Benchmarking      ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                                          ‚ñº
                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                  ‚îÇ Ratio Engine       ‚îÇ
                                  ‚îÇ (Dynamic + SQL UDF)‚îÇ
                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                                             ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ Narrative       ‚îÇ
                                    ‚îÇ Generator (LLM) ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ                                         ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Static Report   ‚îÇ                     ‚îÇ Interactive Dash ‚îÇ
                  ‚îÇ Generator       ‚îÇ                     ‚îÇ (Streamlit)      ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ                                          ‚îÇ
                          ‚ñº                                          ‚ñº
                   S3/GitHub Pages                            User Alerts
                   & PDF/HTML Artifacts                       (Slack/Email)
```

---

## 4. Master Prompt 2.0 (for Code-Orchestration AI)

```
You are ‚ÄúFinBot2CodeGen,‚Äù an autonomous AI engineer. Your task is to generate a full end-to-end, production‚Äêgrade Python/Kubernetes project called `finbot2` that:

1. Defines an Apache Airflow (or Prefect) DAG to schedule daily runs per a dynamic ticker list.
2. Implements microservices for:  
   a) Multi-source ingestion (EDGAR, IR pages, Alpha Vantage, Yahoo Finance).  
   b) Parsing HTML/PDF/XBRL with headless Chrome option for JS pages.  
3. Stores raw files in S3 (versioned) and staging data in PostgreSQL (with timeseries schema).
4. Executes a Great Expectations validation suite + ML anomaly detection on each line-item.
5. Normalizes data via DB-driven chart-of-accounts mapping + currency/date standardization.
6. Populates peer groups (NAICS/SIC) in DB; computes dynamic & user-defined ratios (SQL UDFs or Python).
7. Generates an LLM-powered, citation-aware narrative in English, Spanish & Chinese using a fine-tuned model.
8. Renders:  
   ‚Ä¢ Static Markdown/HTML/PDF reports (Jinja2 + WeasyPrint).  
   ‚Ä¢ An interactive Streamlit dashboard with filters, charts, narrative drill-downs.  
   ‚Ä¢ Push alerts to Slack/Email on threshold breaches.
9. Deploys each component as a Docker container on Kubernetes (Helm charts included).
10. Integrates Prometheus metrics, Grafana dashboard config, ELK‚Äêstyle logging, and Vault secrets.
11. Includes full CI/CD via GitHub Actions: lint (Black/Flake8), type‚Äêcheck (mypy), tests (pytest + testcontainers), security scans (Bandit/Snyk), image publish, Helm upgrade.
12. Provides clear docstrings, type annotations, a comprehensive `README.md`, and examples.

Produce all code, configuration files, Dockerfiles, Helm charts, SQL migrations, CI workflows, and documentation.  
```

---

With this blueprint, **FinBot 2.0** becomes the gold standard in automated financial analysis‚Äîenterprise-ready, globally scalable, and bullet-proof accurate. üöÄ  
Let me know which component you‚Äôd like to deep-dive into next!

---
# FinBot  
*Automated Financial Analysis Pipeline for Public Companies*  

[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/) [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE) [![CI](https://github.com/you/finbot/actions/workflows/ci.yml/badge.svg)](https://github.com/you/finbot/actions)

---

## Table of Contents

1. [Overview](#overview)  
2. [Motivation & Vision](#motivation--vision)  
3. [Key Features](#key-features)  
4. [High-Level Architecture](#high-level-architecture)  
5. [Deep-Dive Workflow](#deep-dive-workflow)  
   1. [1. Source Discovery](#1-source-discovery)  
   2. [2. Scraping & Parsing](#2-scraping--parsing)  
   3. [3. Data Normalization](#3-data-normalization)  
   4. [4. Ratio Calculation](#4-ratio-calculation)  
   5. [5. Narrative Analysis](#5-narrative-analysis)  
   6. [6. Report Generation](#6-report-generation)  
   7. [7. CLI & API Orchestration](#7-cli--api-orchestration)  
   8. [8. Logging, Testing & CI/CD](#8-logging-testing--cicd)  
6. [The Master Prompt](#the-master-prompt)  
7. [Installation & Setup](#installation--setup)  
8. [Usage Examples](#usage-examples)  
   1. [CLI](#cli)  
   2. [REST API](#rest-api)  
   3. [Docker](#docker)  
9. [Configuration & Extensibility](#configuration--extensibility)  
10. [Testing & Quality Assurance](#testing--quality-assurance)  
11. [Contributing](#contributing)  
12. [License](#license)  
13. [Acknowledgements](#acknowledgements)  

---

## Overview  

**FinBot** is a turnkey, end-to-end solution that automatically fetches the latest financial statements of any publicly traded company, parses and normalizes the raw data, computes a comprehensive suite of accounting ratios, generates a narrative investment analysis, and produces a polished report in Markdown, HTML, or PDF‚Äîall with zero manual intervention.  

FinBot was designed for:  
- Financial analysts who want to accelerate their fundamental research.  
- Portfolio managers who need up-to-date metrics and narratives on dozens or hundreds of tickers.  
- Data teams building dashboards or embedding financial KPIs into custom applications.  

By leveraging open-source Python libraries (BeautifulSoup, pdfplumber, sec-edgar-downloader), a robust modular architecture, and state-of-the-art LLMs (e.g., OpenAI GPT-4), FinBot delivers a production-grade pipeline that can be deployed on personal machines, servers, or cloud containers.  

---

## Motivation & Vision  

In today‚Äôs fast-moving markets, the ability to digest and act on financial filings within hours of publication is a competitive edge. Yet most fundamental analysis workflows remain semi-manual:  
- Manually downloading 10-K/10-Q PDFs from SEC or company sites  
- Copy-pasting tables into spreadsheets  
- Writing ratio formulas by hand  
- Drafting commentary from scratch  

These manual steps are error-prone, time-consuming, and don‚Äôt scale. FinBot‚Äôs vision is to fully automate the **complete** process, from data ingestion through report delivery.  

Our core design principles:  
1. **Accuracy & Validation**: Rigorous schema mapping & logging to catch anomalies.  
2. **Modularity & Extensibility**: Swap or extend any component‚Äîparsing engine, ratio set, narrative template.  
3. **Production-Ready**: CLI, REST API, Docker support, CI/CD tests, clear documentation.  
4. **Zero-Touch**: Once configured, run `finbot_cli.py --ticker TSLA` and get a full investor report in minutes.

---

## Key Features  

- üîç **Automated Source Discovery**  
  ‚Ä¢ SEC EDGAR & investor-relations page detection by ticker  
  ‚Ä¢ Google search fallback  
- üìÑ **Multi-Format Parsing**  
  ‚Ä¢ HTML filing tables via BeautifulSoup & pandas  
  ‚Ä¢ PDF extraction via pdfplumber/Tabula  
  ‚Ä¢ XBRL ingestion via `sec-edgar-downloader` & XML parsing  
- üîÑ **Data Normalization**  
  ‚Ä¢ Canonical chart of accounts mapping  
  ‚Ä¢ Currency, date alignment & unit standardization  
- üìä **Comprehensive Ratio Suite**  
  ‚Ä¢ Liquidity, solvency, profitability, efficiency, market multiples  
  ‚Ä¢ Customizable ratio functions  
- ü§ñ **LLM-Powered Narrative**  
  ‚Ä¢ GPT-4 bullet-point summaries: trends, red flags, investment signals  
- üìë **Report Generation**  
  ‚Ä¢ Markdown + embedded charts (Matplotlib/Plotly)  
  ‚Ä¢ Optional HTML/PDF via Jinja2 + WeasyPrint / pdfkit  
- üöÄ **Orchestration Interfaces**  
  ‚Ä¢ CLI (`finbot_cli.py`)  
  ‚Ä¢ REST API (FastAPI)  
  ‚Ä¢ Dockerized container  
- üõ† **Production Essentials**  
  ‚Ä¢ Robust error handling & logging  
  ‚Ä¢ Unit & integration tests  
  ‚Ä¢ GitHub Actions CI  
  ‚Ä¢ Clear docstrings & type hints  

---

## High-Level Architecture  

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ
‚îÇ 1. Discovery  ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ 2. Scraping   ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ 3. Parsing    ‚îÇ
‚îÇ    Module     ‚îÇ    ‚îÇ & Downloading ‚îÇ    ‚îÇ  Module       ‚îÇ
‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                    ‚îÇ
                            ‚îÇ Raw HTML/PDF/XML   ‚îÇ
                            ‚ñº                    ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ
                       ‚îÇ 4. Normalizer ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ 5. Ratios     ‚îÇ
                       ‚îÇ  Module       ‚îÇ    ‚îÇ  Module       ‚îÇ
                       ‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ                    ‚îÇ
                             ‚îÇ Canonical Data     ‚îÇ
                             ‚ñº                    ‚îÇ
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ                                 ‚îÇ
                       ‚îÇ 6. Narrative Analysis (LLM)     ‚îÇ
                       ‚îÇ                                 ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                               ‚îÇ               ‚îÇ
                               ‚îÇ 7. Report     ‚îÇ
                               ‚îÇ  Generation   ‚îÇ
                               ‚îÇ               ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ  CLI  &  REST API   ‚îÇ
                            ‚îÇ  Orchestration      ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Deep-Dive Workflow  

Below is a phase-by-phase exploration of FinBot‚Äôs inner workings.

### 1. Source Discovery  

The first challenge is locating the correct financial filings for a given ticker:

1. **SEC EDGAR Browse**  
   - Query `https://www.sec.gov/edgar/browse/?CIK={TICKER}`.  
   - If the page exists and lists filings, retrieve links to the latest 10-K and 10-Q filings.  

2. **Investor Relations Page**  
   - If EDGAR fails, perform a Google search (via a custom-wired Search API) for `"TICKER Investor Relations"`.  
   - Scrape the IR landing page to find ‚ÄúAnnual Report,‚Äù ‚Äú10-K,‚Äù or ‚ÄúPDF‚Äù links.  

3. **Fallback & Logging**  
   - If neither method succeeds, log an error, retry with exponential backoff, or abort with a descriptive message.

**Key Considerations**  
- Rate-limit SEC requests to comply with their robots policy.  
- Cache discovered URLs (Redis or local JSON) to avoid repeated lookups.  

---

### 2. Scraping & Parsing  

Once URLs are known, FinBot uses specialized parsers:

#### 2.1 HTML Parser  
- Uses **BeautifulSoup** + **pandas.read_html**.  
- Heuristics detect tables whose captions or headers include keywords:  
  > Consolidated, Statement of Operations, Balance Sheet, Cash Flows  

- Extracts each table into a `pandas.DataFrame`.

#### 2.2 PDF Parser  
- Uses **pdfplumber** for text & table extraction.  
- Each page‚Äôs tables are detected; rows & cols reconstructed into DataFrames.  
- Tabula (via `tabula-py`) is an optional alternative for complex layouts.

#### 2.3 XBRL Parser  
- Uses `sec-edgar-downloader` to fetch the latest XBRL `.xml`.  
- Parses via `xml.etree.ElementTree` (or **arelle** for deeper taxonomy support).  
- Extracts line-item numeric facts mapped to US-GAAP elements.

**Edge Cases & Tips**  
- Some companies publish HTML with embedded CSS styling‚Äîuse a robust parser like lxml.  
- PDFs may have multi-column layouts; Tabula‚Äôs area-based extraction can help.  
- Always validate row/column counts and data types before normalization.

---

### 3. Data Normalization  

Raw Data ‚Üí **Canonical Schema**:

- Define a master mapping:
  
  ```python
  STANDARD_ACCOUNTS = {
    "Revenue": ["Total Revenues", "Net Sales", "Revenues"],
    "COGS":    ["Cost of Goods Sold", "Cost of Sales"],
    "Gross Profit": ["Gross Profit"],
    # ‚Ä¶ more mappings ‚Ä¶
  }
  ```

- For each parsed DataFrame:
  1. Scan column headers for aliases.  
  2. Extract the latest period‚Äôs values.  
  3. Convert strings (e.g. ‚Äú$5,123,456‚Äù) ‚Üí floats in millions USD.  
  4. Align period dates to a standard `YYYY-MM-DD` format.

- Assemble a flat `pandas.DataFrame`:

  | item          | period     | value (USD millions) |
  |---------------|------------|----------------------|
  | Revenue       | 2023-12-31 | 394,328              |
  | COGS          | 2023-12-31 | 233,715              |
  | Gross Profit  | 2023-12-31 | 160,613              |
  | ‚Ä¶             | ‚Ä¶          | ‚Ä¶                    |

- Validate completeness: ensure all required accounts are present or flag missing items.

---

### 4. Ratio Calculation  

Implement a library of ratio functions, each annotated and tested:

```python
def current_ratio(df: pd.DataFrame) -> float:
    ca = df.loc[df.item=="Current Assets", "value"].iloc[0]
    cl = df.loc[df.item=="Current Liabilities", "value"].iloc[0]
    return ca / cl

def debt_to_ebitda(df: pd.DataFrame) -> float:
    total_debt = df.loc[df.item=="Total Debt", "value"].iloc[0]
    ebitda = df.loc[df.item=="EBITDA", "value"].iloc[0]
    return total_debt / ebitda
```

**Standard Ratio Categories**:

| Category       | Examples                                                 |
|----------------|----------------------------------------------------------|
| Liquidity      | Current Ratio, Quick Ratio                              |
| Solvency       | Debt/EBITDA, Interest Coverage Ratio                     |
| Profitability  | Gross Margin, Net Profit Margin, ROE, ROA                |
| Efficiency     | Asset Turnover, Inventory Turnover                       |
| Market Metrics | Price/Earnings, EV/EBITDA, EV/Sales                      |
| Custom         | User-defined formulas via plugin functions               |

- `compute_all(df)` orchestrates calls to each function, returning a dictionary:

  ```json
  {
    "Current Ratio": 1.45,
    "Quick Ratio": 1.12,
    "Gross Margin": 0.41,
    "Debt/EBITDA": 3.2,
    ‚Ä¶
  }
  ```

- Include unit tests for each function with known inputs & outputs.

---

### 5. Narrative Analysis  

With ratios in hand, craft an LLM prompt:

```text
You are a seasoned equity analyst. 

Given the following ratios for {TICKER} over the last 3 years:
{RATIOS_JSON}

1. Highlight key trends (improving, deteriorating metrics).  
2. Identify any red flags (rapid debt growth, margin compression).  
3. Note any strengths or competitive advantages.  
4. Provide a high-level buy/hold/sell signal rationale.

Write a concise bullet-point summary (6‚Äì10 points).
```

- Call the OpenAI ChatCompletion API (or equivalent).  
- Post-process the response: ensure bullet formatting, remove hallucinations.  
- Cache narrative outputs for reproducibility.

---

### 6. Report Generation  

Assemble visuals + text:

1. **Charts**  
   - Matplotlib or Plotly to render time series of key ratios.  
   - Save as PNGs in `reports/` directory.

2. **Markdown Templating**  
   - Use **Jinja2** to fill a master template:

     ```jinja
     # {{ ticker }} Financial Analysis Report

     ## 1. Executive Summary  
     **Date:** {{ date }}

     ## 2. Key Ratios  
     | Ratio            | Value     |
     |------------------|-----------|
     {% for name, value in ratios.items() -%}
     | {{ name }}       | {{ "%.2f"|format(value) }} |
     {% endfor %}

     ## 3. Narrative  
     {{ narrative }}

     ## 4. Charts  
     {% for chart in charts -%}
     ![{{ chart.name }}]({{ chart.path }})
     {% endfor %}
     ```

3. **Output Formats**  
   - **Markdown**: `reports/{TICKER}.md`  
   - **PDF**: via `pdfkit.from_string` or WeasyPrint  
   - **HTML**: optional, via MkDocs or direct rendering  

4. **Artifact Organization**  
   - `reports/{TICKER}/` containing:
     - `report.md`  
     - `report.pdf`  
     - `charts/` directory  

---

### 7. CLI & API Orchestration  

**CLI (`finbot_cli.py`)**  
```bash
Usage: python finbot_cli.py --ticker <TICKER> [--format md|pdf|html] [--verbose]
```

- Parses arguments with **Click**.  
- Runs the core pipeline modules in sequence.  
- Exits with status code `0` on success, non-zero on failure.

**REST API (`finbot_api.py`)**  
```http
POST /analyze
Content-Type: application/json

{ "ticker": "AAPL", "format": "pdf" }
```

- Built with **FastAPI**.  
- Async endpoint calls the same internal functions.  
- Returns JSON with `status`, `report_path`, and optional `narrative`.

**Best Practices**  
- Validate inputs via Pydantic schemas.  
- Stream logs to console & file.  
- Use dependency injection for testable components.

---

### 8. Logging, Testing & CI/CD  

- **Logging**  
  - Python‚Äôs `logging` module with multi-level logs (DEBUG, INFO, WARNING, ERROR).  
  - Log files per run in `logs/` with timestamps.

- **Testing**  
  - Unit tests for each parser, normalizer, ratio function.  
  - Integration tests using real tickers (e.g., AAPL, MSFT, TSLA) to validate end-to-end.  
  - Store sample filings under `tests/fixtures/` for offline testing.

- **CI/CD**  
  - GitHub Actions workflow runs on `push` & `pull_request`.  
  - Steps:  
    1. Checkout code  
    2. Setup Python 3.10  
    3. Install dependencies  
    4. Run `pytest --maxfail=1`  
    5. Lint with `flake8` / `black --check`  

- **Code Quality**  
  - Type annotations enforced by `mypy`.  
  - Format enforcement via `black`.  
  - Linting via `flake8`.

---

## The Master Prompt  

> This is the prompt you would feed into your coding-orchestration agent (e.g. Claude Code, LangChain) to generate the entire FinBot codebase automatically.

```
You are ‚ÄúFinBotCodeGen,‚Äù a coding AI specialized in building robust, modular Python pipelines.

Your mission: Generate a full Python project named `finbot` that:

1. Discovers and downloads the latest financial filings (HTML, PDF, XBRL) for any given ticker or company URL.
2. Parses raw filings into structured DataFrames (HTML tables, PDF tables, XBRL facts).
3. Normalizes extracted data into a canonical chart of accounts, units in USD millions, and aligned dates.
4. Calculates a comprehensive suite of ratios (liquidity, solvency, profitability, efficiency, market multiples).
5. Uses a large-language model (e.g. GPT-4) to generate an investment-grade narrative analysis.
6. Renders a polished investor report in Markdown, HTML, and/or PDF, with embedded charts and tables.
7. Exposes both a CLI (`finbot_cli.py`) and a FastAPI REST endpoint (`/analyze`).
8. Includes robust error handling, logging, unit & integration tests, CI/CD (GitHub Actions), and Docker support.
9. Adhere to best practices: Python 3.10+, type hints, docstrings, modular design, clear project structure, comprehensive README.

Output:  
- `finbot/` Python package with submodules: `source/`, `parser/`, `normalize.py`, `ratios.py`, `analysis.py`, `report.py`, `utils.py`.  
- `finbot_cli.py`, `finbot_api.py` entry-points.  
- `requirements.txt`, `Dockerfile`, `.github/workflows/ci.yml`.  
- `tests/` with fixtures and test cases.  
- A thorough `README.md` describing installation, usage, architecture, and examples.

Generate all files, folders, and content.  
```

Use this Master Prompt to spin up an end-to-end code generator that produces the entire FinBot repository.  

---

## Installation & Setup  

Prerequisites:  
- Python 3.10+  
- Git  
- (Optional) Docker, Docker Compose  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/you/finbot.git
   cd finbot
   ```

2. **Create & activate virtual environment**  
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   .\.venv\Scripts\activate   # Windows
   ```

3. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Run tests**  
   ```bash
   pytest
   ```

5. **Set your OpenAI API key**  
   ```bash
   export OPENAI_API_KEY="your_api_key_here"
   ```

---

## Usage Examples  

### CLI  

Generate a Markdown + PDF report for Apple (AAPL):

```bash
python finbot_cli.py --ticker AAPL --format pdf
```

Result:  
```
‚úÖ Parsed filings for AAPL  
‚úÖ Normalized 42 line items  
‚úÖ Computed 15 ratios  
‚úÖ Generated narrative analysis  
‚úÖ Report saved to reports/AAPL/report.pdf  
```

### REST API  

Start the FastAPI server:

```bash
uvicorn finbot_api:app --reload --host 0.0.0.0 --port 8000
```

Call the endpoint:

```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{"ticker":"MSFT","format":"pdf"}'
```

Response:

```json
{
  "status": "done",
  "report": "reports/MSFT/report.pdf"
}
```

### Docker  

1. **Build image**  
   ```bash
   docker build -t finbot:latest .
   ```

2. **Run container**  
   ```bash
   docker run -p 8000:8000 finbot:latest
   ```

3. **Analyze**  
   ```bash
   docker exec -it <container_id> \
     python finbot_cli.py --ticker TSLA --format md
   ```

---

## Configuration & Extensibility  

- **Add New Ratios**  
  1. Create a new function in `finbot/ratios.py` with signature `def my_ratio(df: DataFrame) -> float:`  
  2. Add it to `compute_all()`.

- **Extend Parsers**  
  - Drop in a new parser module under `finbot/parser/`.  
  - Register it in the orchestration logic in `finbot/source/discover.py`.

- **Custom Narrative Template**  
  - Edit the Jinja2 template in `finbot/analysis.py` or swap to another prompt engine.

- **API Authentication**  
  - Integrate OAuth2, API key header, or JWT in `finbot_api.py`.

- **Caching & Speed**  
  - Add Redis/Memcached caching for parsed filings or narrative outputs.  
  - Parallelize ticker processing via multiprocessing or Celery.

---

## Testing & Quality Assurance  

- Run `pytest` regularly; new parsers must include unit tests.  
- Ensure 100% coverage for core modules.  
- Lint with:

  ```bash
  black --check .
  flake8 .
  mypy .
  ```

- Review logs under `logs/` for any parsing or normalization warnings.

---

## Contributing  

Contributions are welcome! To contribute:

1. Fork the repo.  
2. Create a new feature branch (`git checkout -b feature/you-are-awesome`).  
3. Implement your feature or fix.  
4. Add tests and update documentation.  
5. Submit a Pull Request.  

Please follow our [Code of Conduct](CODE_OF_CONDUCT.md).

---

## License  

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## Acknowledgements  

- BeautifulSoup, pdfplumber, pandas, sec-edgar-downloader  
- OpenAI GPT-4 for narrative generation  
- FastAPI for the REST interface  
- Click for the CLI  
- Jinja2 for templating  
- Matplotlib & Plotly for charts  
- Our amazing open-source community!  

---

Thank you for choosing **FinBot**‚Äîthe ultimate zero-touch financial analysis engine. üöÄ  
We can‚Äôt wait to see what insights you‚Äôll uncover!

---
Create two pieces of output:

1. A very brief summary of my internal reasoning (so you can see my guardrails at work), but **without** revealing every hidden step.  
2. A ready-to-copy **‚ÄúMaster Prompt‚Äù** you can feed to a coding-orchestration agent (e.g. Claude Code, LangChain, etc.) to build a fully automated financial analysis pipeline.

---

**1. High-Level Rationale (concise)**  
I‚Äôve broken the problem into six phases‚Äîsource discovery, scraping/parsing, data normalization, ratio calculation, narrative analysis, and reporting. For each phase I selected robust, battle-tested tools (e.g. Python + BeautifulSoup/PDF parser/XBRL, plus the LLM for semantic extraction and narrative). I then wove them into a single master prompt that orchestrates orchestration code generation, error-handling, and modular extensibility.

---

**2. Master Prompt for Automated Financial-Analysis Pipeline**  
```
You are ‚ÄúFinBot,‚Äù a coding AI assistant whose sole task is to build a turnkey pipeline that, given any publicly traded company ticker or URL, will:

  ‚Ä¢ Discover and download the latest annual/quarterly financial statements  
  ‚Ä¢ Parse HTML/PDF/XBRL filings to extract line-item data  
  ‚Ä¢ Normalize that data into a standard schema  
  ‚Ä¢ Compute a comprehensive set of accounting ratios (liquidity, solvency, profitability, efficiency, market multiples)  
  ‚Ä¢ Analyze trends and flag potential red flags or highlights  
  ‚Ä¢ Produce an investor-grade report (Markdown/HTML/PDF) with tables, charts, and narrative  
  ‚Ä¢ Log all steps, errors, and data-validation notices  

You must generate production-quality Python code with:  
 1. A modular architecture (one module per phase above).  
 2. Robust error-handling and retries for network requests.  
 3. Clear docstrings, type annotations, and a CI-friendly project structure.  
 4. Integration tests (e.g. sample tickers from NYSE/NASDAQ).  
 5. A final CLI or REST entry point (e.g. `python finbot.py --ticker AAPL` or `POST /analyze {ticker:"AAPL"}`).

Detailed Requirements:

1. Source Discovery  
   - Given a ticker or URL, find the company‚Äôs Investor Relations page or SEC EDGAR link.  
   - Fallback to Google Search API when direct links fail.

2. Scraping & Parsing  
   - For HTML pages: use BeautifulSoup to locate ‚ÄúAnnual Report,‚Äù ‚Äú10-K,‚Äù ‚Äú10-Q.‚Äù  
   - For PDFs: use pdfplumber or PyPDF2 to extract text, then heuristic tables detection or Tabula.  
   - For XBRL: use `arelle` or `sec-edgar-downloader` + lxml to pull structured data.

3. Data Normalization  
   - Map extracted line items (e.g. ‚ÄúTotal Revenue,‚Äù ‚ÄúRevenues,‚Äù ‚ÄúNet Sales‚Äù) to a canonical chart of accounts.  
   - Convert all numbers to millions USD (or local currency) and enforce date alignment.

4. Ratio Calculation  
   - Implement functions for:  
     * Liquidity: current ratio, quick ratio  
     * Solvency: debt/EBITDA, interest coverage  
     * Profitability: gross margin, ROE, ROA, net margin  
     * Efficiency: asset turnover, inventory turnover  
     * Market multiples: P/E, EV/EBITDA  
   - Accept optional parameters for custom ratios.

5. Narrative Analysis  
   - Use the LLM to generate a bullet-point summary of:  
     * Year-over-year and quarter-over-quarter changes  
     * Outliers or restatements  
     * Peer comparisons if tickers list provided  
   - Prompt template example:  
     ‚ÄúGiven these ratios for [COMPANY] from 2019-2023, highlight trends, risks, and investment signals.‚Äù

6. Report Generation  
   - Assemble charts (Matplotlib/Plotly) for key ratios.  
   - Export final report as:  
     * Markdown + embedded images  
     * Optionally render as HTML or PDF via `weasyprint` or `mkdocs-pdf`.

7. Logging & CI  
   - All HTTP requests, parse failures, and normalization mismatches must be logged.  
   - Include a `tests/` folder with at least 3 real‚Äêworld examples that validate end‚Äêto‚Äêend functionality.

Task for Claude Code:

‚ÄúWrite a complete Python project named `finbot` satisfying the above requirements.  
Structure it as a CLI tool and a REST API (FastAPI).  
Include a `README.md` explaining setup, usage examples, and how to add new ratio modules.  
Ensure no human prompts are needed at runtime‚Äîonce set up, the user just supplies a ticker or URL and the system outputs the finished report.‚Äù

‚Äî‚Äî

Feed this Master Prompt to your coding agent; it will generate all modules, tests, and CI configuration needed for a production-grade, fully automated financial analysis pipeline. Good luck!

